import logging
import time
import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import linear_model
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV

from mlstock.data import data_filter, data_loader
from mlstock.data.datasource import DataSource
from mlstock.data.stock_info import StocksInfo
from mlstock.factors.daily_indicator import DailyIndicator
from mlstock.factors.kdj import KDJ
from mlstock.factors.macd import MACD
from mlstock.factors.psy import PSY
from mlstock.factors.rsi import RSI
from mlstock.factors.balance_sheet import BalanceSheet
from mlstock.factors.cashflow import CashFlow
from mlstock.factors.income import Income
from mlstock.factors.std import Std
from mlstock.factors.returns import Return
from mlstock.factors.turnover_return import TurnoverReturn
from mlstock.utils import utils
from mlstock.utils.utils import time_elapse

warnings.filterwarnings("ignore")
from sklearn.preprocessing import StandardScaler

logger = logging.getLogger(__name__)

FACTORS = [Return, TurnoverReturn, Std, MACD, KDJ, PSY, RSI, BalanceSheet, Income, CashFlow, DailyIndicator]


def main(start_date, end_date, num):
    start_time = time.time()
    datasource = DataSource()

    # è¿‡æ»¤éä¸»æ¿ã€éä¸­å°æ¿è‚¡ç¥¨ã€ä¸”ä¸Šå¸‚åœ¨1å¹´ä»¥ä¸Šçš„éSTè‚¡ç¥¨
    df_stock_basic = data_filter.filter_stocks()
    df_stock_basic = df_stock_basic.iloc[:num]
    stocks_info = StocksInfo(df_stock_basic.ts_code, start_date, end_date)

    # ä¸´æ—¶ä¿å­˜ä¸€ä¸‹ï¼Œç”¨äºæœ¬åœ°ä¸‹è½½æ•°æ®æä¾›åˆ—è¡¨ï¼ˆè°ƒè¯•ç”¨ï¼‰
    df_stock_basic.ts_code.to_csv("data/stocks.txt", index=False)

    # åŠ è½½å‘¨é¢‘æ•°æ®
    stock_data = data_loader.load(datasource, df_stock_basic.ts_code, start_date, end_date)

    # æŠŠåŸºç¡€ä¿¡æ¯mergeåˆ°å‘¨é¢‘æ•°æ®ä¸­
    df_weekly = stock_data.df_weekly.merge(df_stock_basic, on='ts_code', how='left')

    # æŸåªè‚¡ç¥¨ä¸Šå¸‚12å‘¨å†…çš„æ•°æ®æ‰”æ‰ï¼Œä¸éœ€è¦
    old_length = len(df_weekly)
    a = pd.to_datetime(df_weekly.trade_date, format='%Y%m%d')
    b = pd.to_datetime(df_weekly.list_date, format='%Y%m%d')
    df_weekly = df_weekly[a - b > pd.Timedelta(12, unit='w')]
    logger.info("å‰”é™¤æ‰ä¸Šå¸‚12å‘¨å†…çš„æ•°æ®ï¼š%d=>%d", old_length, len(df_weekly))

    factor_names = []
    # è·å–æ¯ä¸€ä¸ªå› å­ï¼ˆç‰¹å¾ï¼‰ï¼Œå¹¶ä¸”ï¼Œå¹¶å…¥åˆ°è‚¡ç¥¨æ•°æ®ä¸­
    for factor_class in FACTORS:
        factor = factor_class(datasource, stocks_info)
        df_factor = factor.calculate(stock_data)
        df_weekly = factor.merge(df_weekly, df_factor)
        factor_names += factor.name if type(factor.name) == list else [factor.name]
        logger.info("è·å–å› å­%r %d è¡Œæ•°æ®", factor.name, len(df_factor))

    logger.info("å› å­è·å–å®Œæˆï¼Œåˆè®¡%dä¸ªå› å­%rï¼Œ%d è¡Œæ•°æ®", len(factor_names), factor_names, len(df_weekly))

    # å› ä¸ºå‰é¢çš„æ—¥æœŸä¸­ï¼Œä¸ºäº†é˜²æ­¢MACDä¹‹ç±»çš„æŠ€æœ¯æŒ‡æ ‡å‡ºç°NANé¢„åŠ è½½äº†æ•°æ®ï¼Œæ‰€ä»¥è¦è¿‡æ»¤æ‰è¿™äº›start_dateä¹‹å‰çš„æ•°æ®
    original_length = len(df_weekly)
    df_weekly = df_weekly[df_weekly.trade_date >= start_date]
    logger.debug("è¿‡æ»¤æ‰[%s]ä¹‹å‰çš„æ•°æ®ï¼ˆä¸ºé˜²æ­¢æŠ€æœ¯æŒ‡æ ‡nanï¼‰åï¼š%d => %d è¡Œ", start_date, original_length, len(df_weekly))

    # åˆå¹¶æ²ªæ·±300çš„å‘¨æ”¶ç›Šç‡ï¼Œä¸ºä½•ç”¨å®ƒå‘¢ï¼Œæ˜¯ä¸ºäº†è®¡ç®—è¶…é¢æ”¶ç›Š(r_i = pct_chg - pct_chg_hs300)
    df_hs300 = datasource.index_weekly("000300.SH", start_date, end_date)
    df_hs300 = df_hs300[['trade_date', 'pct_chg']]
    df_hs300 = df_hs300.rename(columns={'pct_chg': 'pct_chg_hs300'})
    logger.info("ä¸‹è½½æ²ªæ·±300 %s~%s æ•°æ® %d æ¡", start_date, end_date, len(df_hs300))

    df_weekly = df_weekly.merge(df_hs300, on=['trade_date'], how='left')
    logger.info("åˆå¹¶æ²ªæ·±300 %d=>%d", len(df_weekly), len(df_weekly))

    # è®¡ç®—å‡ºå’ŒåŸºå‡†ï¼ˆæ²ªæ·±300ï¼‰çš„è¶…é¢æ”¶ç›Šç‡ï¼Œå¹¶ä¸”åŸºäºå®ƒï¼Œè®¾ç½®é¢„æµ‹æ ‡ç­¾'target'ï¼ˆé¢„æµ‹ä¸‹ä¸€æœŸï¼Œæ‰€ä»¥åšshiftï¼‰

    df_weekly['rm_rf'] = df_weekly.pct_chg - df_weekly.pct_chg_hs300
    # targetå³é¢„æµ‹ç›®æ ‡ï¼Œæ˜¯ä¸‹ä¸€æœŸçš„è¶…é¢æ”¶ç›Š
    df_weekly['target'] = df_weekly.groupby('ts_code').rm_rf.shift(-1)

    """
    æ¯ä¸€åˆ—ï¼Œéƒ½å»æå€¼ï¼ˆTODOï¼šæ˜¯ä¸æ˜¯æŒ‰ç…§å„è‚¡è‡ªå·±çš„å€¼æ¥åšæ˜¯ä¸æ˜¯æ›´å¥½ï¼Ÿç°åœ¨æ˜¯æ‰€æœ‰çš„è‚¡ç¥¨ï¼‰
    ä¸­ä½æ•°å»æå€¼:
    - è®¾ç¬¬ T æœŸæŸå› å­åœ¨æ‰€æœ‰ä¸ªè‚¡ä¸Šçš„æš´éœ²åº¦åºåˆ—ä¸ºğ·ğ‘–
    - ğ·ğ‘€ä¸ºè¯¥åºåˆ—ä¸­ä½æ•°
    - ğ·ğ‘€1ä¸ºåºåˆ—|ğ·ğ‘– âˆ’ ğ·ğ‘€|çš„ä¸­ä½æ•°
    - åˆ™å°†åºåˆ—ğ·ğ‘–ä¸­æ‰€æœ‰å¤§äºğ·ğ‘€ + 5ğ·ğ‘€1çš„æ•°é‡è®¾ä¸ºğ·ğ‘€ + 5ğ·ğ‘€1
    - å°†åºåˆ—ğ·ğ‘–ä¸­æ‰€æœ‰å°äºğ·ğ‘€ âˆ’ 5ğ·ğ‘€1çš„æ•°é‡è®¾ä¸ºğ·ğ‘€ âˆ’ 5ğ·ğ‘€1
    """

    def scaller(x):
        _max = df_median[x.name] + 5 * df_scope[x.name]
        _min = df_median[x.name] - 5 * df_scope[x.name]
        x = x.apply(lambda v: _min if v < _min else v)
        x = x.apply(lambda v: _max if v > _max else v)
        return x

    # ä¿ç•™feature
    df_features = df_weekly[factor_names]
    # æ¯åˆ—éƒ½æ±‚ä¸­ä½æ•°ï¼Œå’Œä¸­ä½æ•°ä¹‹å·®çš„ç»å¯¹å€¼çš„ä¸­ä½æ•°
    df_median = df_features.median()
    df_scope = df_features.apply(lambda x: x - df_median[x.name]).abs().median()
    df_features = df_features.apply(scaller)
    df_weekly[factor_names] = df_features

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    scaler.fit(df_weekly[factor_names])
    df_weekly[factor_names] = scaler.transform(df_weekly[factor_names])
    logger.info("å¯¹%dä¸ªç‰¹å¾è¿›è¡Œäº†æ ‡å‡†åŒ–(ä¸­ä½æ•°å»æå€¼)å¤„ç†ï¼š%d è¡Œ", len(factor_names), len(df_weekly))

    # å»é™¤æ‰€æœ‰çš„NANæ•°æ®
    logger.info("NAç»Ÿè®¡ï¼šæ•°æ®ç‰¹å¾ä¸­çš„NANæ•°ï¼š\n%r", df_weekly[factor_names].isna().sum())
    df_weekly = filter_invalid_data(df_weekly, factor_names)

    df_weekly.dropna(subset=factor_names + ['target'], inplace=True)
    logger.info("å»é™¤NANåï¼Œæ•°æ®å‰©ä½™è¡Œæ•°ï¼š%d è¡Œ", len(df_weekly))

    df_data = df_weekly[['ts_code', 'trade_date'] + factor_names + ['target']]
    csv_file_name = "data/{}_{}_{}.csv".format(start_date, end_date, utils.now())
    df_data.to_csv(csv_file_name, index=False)
    logger.info("ä¿å­˜ %d è¡Œï¼ˆè®­ç»ƒå’Œæµ‹è¯•ï¼‰æ•°æ®åˆ°æ–‡ä»¶ï¼š%s", len(df_data), csv_file_name)
    start_time = time_elapse(start_time, "åŠ è½½æ•°æ®å’Œæ¸…æ´—ç‰¹å¾")

    # å‡†å¤‡è®­ç»ƒç”¨æ•°æ®ï¼Œéœ€è¦numpyç±»å‹
    assert len(df_weekly) > 0
    X_train = df_weekly[factor_names].values
    y_train = df_weekly.target

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæµ‹è¯•é›†å æ€»æ•°æ®çš„15%ï¼Œéšæœºç§å­ä¸º10
    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.15, random_state=10)

    # ä½¿ç”¨äº¤å‰éªŒè¯ï¼Œåˆ†æˆ10ä»½ï¼ŒæŒ¨ä¸ªåšK-Foldï¼Œè®­ç»ƒ
    cv_scores = []
    for n in range(5):
        regession = linear_model.LinearRegression()
        scores = cross_val_score(regession, X_train, y_train, cv=10, scoring='neg_mean_squared_error')
        cv_scores.append(scores.mean())
    logger.info("æˆç»©ï¼š\n%r", cv_scores)

    # åšè¿™ä¸ªæ˜¯ä¸ºäº†äººè‚‰çœ‹ä¸€ä¸‹æœ€å¥½çš„å²­å›å½’çš„è¶…å‚alphaçš„æœ€ä¼˜å€¼æ˜¯å•¥
    # æ˜¯æ²¡å¿…è¦çš„ï¼Œå› ä¸ºåé¢è¿˜ä¼šç”¨ gridsearchè‡ªåŠ¨è·‘ä¸€ä¸‹ï¼Œåšè¿™ä¸ªå°±æ˜¯æƒ³ç›´è§‚çš„æ„Ÿå—ä¸€ä¸‹
    results = []
    alpha_scope = np.arange(200, 500, 5)
    for i in alpha_scope:
        ridge = Ridge(alpha=i)
        results.append(cross_val_score(ridge, X_train, y_train, cv=10, scoring='neg_mean_squared_error').mean())
    logger.info("æœ€å¥½çš„å‚æ•°ï¼š%.0f, å¯¹åº”çš„æœ€å¥½çš„å‡æ–¹è¯¯å·®ï¼š%.2f",
                alpha_scope[results.index(max(results))],
                max(results))
    plt.figure(figsize=(20, 5))
    plt.title('Best Apha')
    plt.plot(alpha_scope, results, c="red", label="alpha")
    plt.legend()
    plt.show()

    # ç”¨grid searchæ‰¾æœ€å¥½çš„alphaï¼š[200,205,...,500]
    # grid searchçš„å‚æ•°æ˜¯alphaï¼Œå²­å›å½’å°±è¿™æ ·ä¸€ä¸ªå‚æ•°ï¼Œç”¨äºçº¦æŸå‚æ•°çš„å¹³æ–¹å’Œ
    # grid searchçš„å…¥å‚åŒ…æ‹¬alphaçš„èŒƒå›´ï¼ŒK-Foldçš„æŠ˜æ•°(cv)ï¼Œè¿˜æœ‰å²­å›å½’è¯„ä»·çš„å‡½æ•°(è´Ÿå‡æ–¹è¯¯å·®)
    grid_search = GridSearchCV(Ridge(),
                               {'alpha': alpha_scope},
                               cv=5,  # 5æŠ˜(KFoldå€¼)
                               scoring='neg_mean_squared_error')
    grid_search.fit(X_train, y_train)
    # model = grid_search.estimator.fit(X_train, y_train)
    logger.info("GridSarchæœ€å¥½çš„æˆç»©:%.5f", grid_search.best_score_)
    # å¾—åˆ°çš„ç»“æœæ˜¯495ï¼Œç¡®å®å’Œä¸Šé¢äººè‚‰è·‘æ˜¯ä¸€æ ·çš„ç»“æœ
    logger.info("GridSarchæœ€å¥½çš„å‚æ•°:%.5f", grid_search.best_estimator_.alpha)


def filter_invalid_data(df, factor_names):
    for factor_name in factor_names:
        original_size = len(df)
        # å»æ‰é‚£äº›è¿™ä¸ªç‰¹å¾å…¨æ˜¯nançš„è‚¡ç¥¨
        valid_ts_codes = df.groupby('ts_code')[factor_name].count()[lambda x: x > 0].index
        df = df[df['ts_code'].isin(valid_ts_codes)]
        if len(df) != original_size:
            logger.info("å»é™¤ç‰¹å¾[%s]å…¨éƒ¨ä¸ºNançš„è‚¡ç¥¨æ•°æ®åï¼Œè¡Œæ•°å˜åŒ–ï¼š%d => %d",
                        factor_name, original_size, len(df))
    return df


# python -m mlstock.ml.train
if __name__ == '__main__':
    utils.init_logger(file=False, log_level=logging.INFO)
    start_date = "20180101"
    end_date = "20220101"
    num = 20
    main(start_date, end_date, num)
