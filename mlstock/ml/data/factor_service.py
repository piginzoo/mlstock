import argparse
import json
import logging
import os
import time

import pandas as pd
from sklearn.preprocessing import StandardScaler

from mlstock.const import CODE_DATE, BASELINE_INDEX_CODE, TARGET
from mlstock.data import data_filter, data_loader
from mlstock.data.datasource import DataSource
from mlstock.data.stock_info import StocksInfo
from mlstock.ml.data import factor_conf
from mlstock.ml.data.factor_conf import FACTORS
from mlstock.utils import utils
from mlstock.utils.industry_neutral import IndustryMarketNeutral
from mlstock.utils.utils import time_elapse

logger = logging.getLogger(__name__)


def calculate(start_date, end_date, num, is_industry_neutral):
    """
    ä»å¤´å¼€å§‹è®¡ç®—å› å­
    :param start_date:
    :param end_date:
    :param num:
    :return:
    """

    start_time = time.time()
    data_source = DataSource()

    # åŠ è½½è‚¡ç¥¨æ•°æ®
    stock_data, ts_codes = load_stock_data(data_source, start_date, end_date, num)

    # åŠ è½½ï¼ˆè®¡ç®—ï¼‰å› å­
    df_weekly, factor_names = calculate_factors(data_source, stock_data, StocksInfo(ts_codes, start_date, end_date))

    # æ˜¾å­˜ä¸€ä»½æœ€åŸå§‹çš„æ•°æ®
    time_elapse(start_time, "â­ï¸ å…¨éƒ¨å› å­åŠ è½½å®Œæˆ")

    # åŠ è½½åŸºå‡†ï¼ˆæŒ‡æ•°ï¼‰æ•°æ®
    df_weekly = prepare_target(df_weekly, start_date, end_date, data_source)

    # æ¸…æ™°å› å­æ•°æ®
    df_weekly = clean_factors(df_weekly, factor_names, start_date, end_date, is_industry_neutral)

    # ä¿å­˜åŸå§‹æ•°æ®å’Œå¤„ç†åçš„æ•°æ®
    # save_csv("raw", df_weekly, start_date, end_date)
    save_csv("processed" + ("_industry_neutral" if is_industry_neutral else ""), df_weekly, start_date, end_date)

    return df_weekly, factor_names


def load_stock_data(data_source, start_date, end_date, num):
    """
    ç­›é€‰å‡ºåˆé€‚çš„è‚¡ç¥¨ï¼Œå¹¶ï¼ŒåŠ è½½æ•°æ®

    :param data_source:
    :param start_date:
    :param end_date:
    :param num:
    :return:
    """

    # è¿‡æ»¤éä¸»æ¿ã€éä¸­å°æ¿è‚¡ç¥¨ã€ä¸”ä¸Šå¸‚åœ¨1å¹´ä»¥ä¸Šçš„éSTè‚¡ç¥¨
    df_stock_basic = data_filter.filter_stocks()
    df_stock_basic = df_stock_basic.iloc[:num]
    df_stock_basic = process_industry(df_stock_basic)  # æŠŠindustryåˆ—æ¢æˆID

    ts_codes = df_stock_basic.ts_code

    # ä¸´æ—¶ä¿å­˜ä¸€ä¸‹ï¼Œç”¨äºæœ¬åœ°ä¸‹è½½æ•°æ®æä¾›åˆ—è¡¨ï¼ˆè°ƒè¯•ç”¨ï¼‰
    # df_stock_basic.ts_code.to_csv("data/stocks.txt", index=False)

    # åŠ è½½å‘¨é¢‘æ•°æ®
    stock_data = data_loader.load(data_source, ts_codes, start_date, end_date)

    # æŠŠåŸºç¡€ä¿¡æ¯mergeåˆ°å‘¨é¢‘æ•°æ®ä¸­
    df_weekly = stock_data.df_weekly.merge(df_stock_basic, on='ts_code', how='left')

    # æŸåªè‚¡ç¥¨ä¸Šå¸‚12å‘¨å†…çš„æ•°æ®æ‰”æ‰ï¼Œä¸éœ€è¦
    old_length = len(df_weekly)
    a = pd.to_datetime(df_weekly.trade_date, format='%Y%m%d')
    b = pd.to_datetime(df_weekly.list_date, format='%Y%m%d')
    df_weekly = df_weekly[a - b > pd.Timedelta(12, unit='w')]
    logger.info("å‰”é™¤æ‰ä¸Šå¸‚12å‘¨å†…çš„æ•°æ®ï¼š%d=>%d", old_length, len(df_weekly))

    stock_data.df_weekly = df_weekly
    return stock_data, ts_codes


def calculate_factors(data_source, stock_data, stocks_info):
    """
    è®¡ç®—æ¯ä¸€ä¸ªå› å­ï¼Œå› å­åˆ—è¡¨æ¥è‡ªäºfactor_conf.py

    :param data_source:
    :param stock_data:
    :param stocks_info:
    :return:
    """

    logger.info("åŠ è½½å’Œæ¸…æ´—æ•°æ®")

    factor_names = []
    df_weekly = stock_data.df_weekly

    # è·å–æ¯ä¸€ä¸ªå› å­ï¼ˆç‰¹å¾ï¼‰ï¼Œå¹¶ä¸”ï¼Œå¹¶å…¥åˆ°è‚¡ç¥¨æ•°æ®ä¸­
    for factor_class in FACTORS:
        factor = factor_class(data_source, stocks_info)
        df_factor = factor.calculate(stock_data)
        df_weekly = factor.merge(df_weekly, df_factor)
        factor_names += factor.name if type(factor.name) == list else [factor.name]
        logger.info("è·å–å› å­%r %d è¡Œæ•°æ®", factor.name, len(df_factor))

    logger.info("å› å­åŠ è½½å®Œæˆï¼Œåˆè®¡ %d è¡Œæ•°æ®ï¼Œ%dä¸ªå› å­:\n%r", len(df_weekly), len(factor_names), factor_names)
    return df_weekly, factor_names


def load_from_file(factors_file_path):
    """
    ä»æ–‡ä»¶ä¸­ï¼Œç›´æ¥åŠ è½½å› å­æ•°æ®

    :param factors_file_path:
    :return:
    """
    if not os.path.exists(factors_file_path):
        raise ValueError(f"å› å­æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{factors_file_path}")
    df_features = pd.read_csv(factors_file_path, header=0)
    df_features['trade_date'] = df_features['trade_date'].astype(str)
    df_features['target'] = df_features['target'].astype(int)
    return df_features


def extract_features(df):
    return df[factor_conf.get_factor_names()]


def extract_train_data(df):
    """
    ä»…ä¿ç•™è®­ç»ƒç”¨çš„åˆ—
    :param factors_file_path:
    :return:
    """
    return df[CODE_DATE + factor_conf.get_factor_names() + TARGET]


def prepare_target(df_weekly, start_date, end_date, datasource):
    """
    è®¡ç®—åŸºå‡†çš„æ”¶ç›Šç‡ç­‰å„ç§é¢„æµ‹ç”¨çš„æ”¶ç›Šç‡
    :param df_weekly:
    :param datasource:
    :return:
    """

    # åˆå¹¶æ²ªæ·±300çš„å‘¨æ”¶ç›Šç‡ï¼Œä¸ºä½•ç”¨å®ƒå‘¢ï¼Œæ˜¯ä¸ºäº†è®¡ç®—è¶…é¢æ”¶ç›Š(r_i = pct_chg - next_pct_chg_baseline)
    df_baseline = datasource.index_weekly(BASELINE_INDEX_CODE, start_date, end_date)
    df_baseline = df_baseline[['trade_date', 'pct_chg']]
    df_baseline = df_baseline.rename(columns={'pct_chg': 'pct_chg_baseline'})
    logger.info("ä¸‹è½½åŸºå‡†[%s] %s~%s æ•°æ® %d æ¡", BASELINE_INDEX_CODE, start_date, end_date, len(df_baseline))

    df_weekly = df_weekly.merge(df_baseline, on=['trade_date'], how='left')
    logger.info("åˆå¹¶åŸºå‡†[%s] %d=>%d", BASELINE_INDEX_CODE, len(df_weekly), len(df_weekly))

    # è®¡ç®—å‡ºå’ŒåŸºå‡†çš„è¶…é¢æ”¶ç›Šç‡ï¼Œå¹¶ä¸”åŸºäºå®ƒï¼Œè®¾ç½®é¢„æµ‹æ ‡ç­¾'target'ï¼ˆé¢„æµ‹ä¸‹ä¸€æœŸï¼Œæ‰€ä»¥åšshiftï¼‰
    df_weekly['rm_rf'] = df_weekly.pct_chg - df_weekly.pct_chg_baseline

    # targetå³é¢„æµ‹ç›®æ ‡ï¼Œæ˜¯"ä¸‹ä¸€æœŸ"çš„è¶…é¢æ”¶ç›Šï¼Œè®­ç»ƒä¸»è¦é è¿™ä¸ªï¼Œä»–å°±æ˜¯è®­ç»ƒçš„y
    df_weekly['target'] = df_weekly.groupby('ts_code').rm_rf.shift(-1)

    # ä¸‹ä¸€æœŸçš„æ”¶ç›Šç‡ï¼Œè¿™ä¸ªæ˜¯ä¸ºäº†å°†æ¥åšå›æµ‹è¯„ä»·ç”¨ï¼Œæ³¨æ„ï¼Œæ˜¯ä¸‹ä¸€æœŸï¼Œæ‰€ä»¥åšäº†shift(-1)
    df_weekly['next_pct_chg'] = df_weekly.groupby('ts_code').pct_chg.shift(-1)
    df_weekly['next_pct_chg_baseline'] = df_weekly.groupby('ts_code').pct_chg_baseline.shift(-1)

    return df_weekly


def _scaller(x, df_median, df_scope):
    """
    - åˆ™å°†åºåˆ—ğ·ğ‘–ä¸­æ‰€æœ‰å¤§äºğ·ğ‘€ + 5ğ·ğ‘€1çš„æ•°é‡è®¾ä¸ºğ·ğ‘€ + 5ğ·ğ‘€1
    - å°†åºåˆ—ğ·ğ‘–ä¸­æ‰€æœ‰å°äºğ·ğ‘€ âˆ’ 5ğ·ğ‘€1çš„æ•°é‡è®¾ä¸ºğ·ğ‘€ âˆ’ 5ğ·ğ‘€1
    :param x: å°±æ˜¯æŸä¸€åˆ—ï¼Œæ¯”å¦‚beta
        Name: beta, Length: 585, dtype: float64
        180          NaN
        181          NaN
                  ...
        1196    163121.0
    :param df_median:
        (Pdb) df_median
        return_1w                 -0.002050
        return_3w                 -0.007407
        .....                     ......
        alpha                     0.000161
        beta                      0.276572
        stake_holder              163121.000000
        Length: 73, dtype: float64
    :param df_scope:
        (Pdb) df_scope
        return_1w                 0.029447
        .....                     ......
        stake_holder              82657.000000
        Length: 73, dtype: float64
    :return:
    """
    _max = df_median[x.name] + 5 * df_scope[x.name]
    _min = df_median[x.name] - 5 * df_scope[x.name]
    x = x.apply(lambda v: _min if v < _min else v)
    x = x.apply(lambda v: _max if v > _max else v)
    return x


def clean_factors(df_weekly, factor_names, start_date, end_date, is_industry_market_neutral):
    """
    å¯¹å› å­æ•°æ®åšè¿›ä¸€æ­¥çš„æ¸…æ´—ï¼Œè¿™æ­¥å¾ˆé‡è¦ï¼Œä¹Ÿå¾ˆæ…¢
    :param df_features:
    :param factor_names:
    :param start_date: å› ä¸ºå‰é¢çš„æ—¥æœŸä¸­ï¼Œä¸ºäº†é˜²æ­¢MACDä¹‹ç±»çš„æŠ€æœ¯æŒ‡æ ‡å‡ºç°NANé¢„åŠ è½½äº†æ•°æ®ï¼Œæ‰€ä»¥è¦è¿‡æ»¤æ‰è¿™äº›start_dateä¹‹å‰çš„æ•°æ®
    :return:
    """

    start_time = time.time()

    """
    å› ä¸ºå‰é¢çš„æ—¥æœŸä¸­ï¼Œä¸ºäº†é˜²æ­¢MACDä¹‹ç±»çš„æŠ€æœ¯æŒ‡æ ‡å‡ºç°NANé¢„åŠ è½½äº†æ•°æ®ï¼Œæ‰€ä»¥è¦è¿‡æ»¤æ‰è¿™äº›start_dateä¹‹å‰çš„æ•°æ®
    """
    original_length = len(df_weekly)
    df_weekly = df_weekly[df_weekly.trade_date >= start_date]
    logger.info("è¿‡æ»¤æ‰[%s]ä¹‹å‰çš„æ•°æ®ï¼ˆä¸ºé˜²æ­¢æŠ€æœ¯æŒ‡æ ‡nanï¼‰åï¼š%d => %d è¡Œ", start_date, original_length, len(df_weekly))

    logger.info("(è°ƒè¯•)ç‰¹å¾å¤„ç†ä¹‹å‰çš„æ•°æ®æƒ…å†µï¼š\n%r", df_weekly[CODE_DATE + factor_names].describe())

    with pd.option_context('display.max_rows', None, 'display.max_columns', None):
        logger.info("(è°ƒè¯•)ç‰¹å¾å¤„ç†ä¹‹å‰NAç»Ÿè®¡ï¼šæ•°æ®ç‰¹å¾ä¸­çš„NANæ•°ï¼š\n%r", df_weekly[factor_names].isna().sum().sort_values())

    """
    å¦‚æœtargetç¼ºå¤±æ¯”è¾ƒå¤šï¼Œå°±åˆ é™¤æ‰è¿™äº›è‚¡ç¥¨
    """
    original_length = len(df_weekly)
    df_weekly = df_weekly[~df_weekly.target.isna()]
    logger.info("è¿‡æ»¤æ‰targetä¸ºnançš„è¡Œåï¼š%d => %d è¡Œï¼Œå‰”é™¤å æ¯”%.1f%%",
                original_length,
                len(df_weekly),
                (original_length - len(df_weekly)) * 100 / original_length)

    """
    å»é™¤é‚£äº›å› å­å€¼ä¸­è¶…è¿‡20%ç¼ºå¤±çš„è‚¡ç¥¨ï¼ˆçœ‹æ‰€æœ‰å› å­ä¸­ç¡®å®æœ€å¤§çš„é‚£ä¸ªï¼Œç™¾åˆ†æ¯”è¶…è¿‡20%ï¼Œè¿™åªè‚¡ç¥¨æ•´ä¸ªå‰”é™¤æ‰ï¼‰
    """
    # è®¡ç®—æ¯åªè‚¡ç¥¨çš„æ¯ä¸ªç‰¹å¾çš„ç¼ºå¤±ç™¾åˆ†æ¯”
    # ä»…ç”¨[ ç‰¹å¾s, è‚¡ç¥¨,æ—¥æœŸ ] è¿™äº›åˆ—ä½œä¸ºç»Ÿè®¡æ‰‹æ®µ
    df_na_miss_percent_by_code = df_weekly[CODE_DATE + factor_names].groupby(by='ts_code').apply(
        lambda df: (df.shape[0] - df.count()) / df.shape[0])

    # æ‰¾å‡ºæœ€å¤§çš„é‚£ä¸ªç‰¹å¾çš„ç¼ºå¤±æ¯”ï¼Œå¦‚æœå…¶>80%ï¼Œå°±å‰”é™¤è¿™åªè‚¡ç¥¨
    df_na_miss_codes = df_na_miss_percent_by_code[df_na_miss_percent_by_code.max(axis=1) > 0.8]['ts_code']
    # æŠŠè¿™äº›è¡Œæ‰¾å‡ºæ¥ï¼Œæ‰“å°åˆ°æ—¥å¿—ä¸­ï¼Œæ–¹ä¾¿åæœŸè°ƒè¯•
    df_missed_info = df_na_miss_percent_by_code[
        df_na_miss_percent_by_code.apply(lambda x: x.name in df_na_miss_codes, axis=1)]
    # 0ç¼ºå¤±çš„åˆ—ï¼Œéœ€è¦æ‰£æ‰ï¼Œåªä¿ç•™ç¡®å®åˆ—æ‰“å°å‡ºæ¥è°ƒè¯•
    need_drop_columns = df_missed_info.sum()[df_missed_info.sum() == 0].index
    # ä»…ä¿ç•™ç¡®å®å­˜åœ¨ç¡®å®çš„åˆ—ï¼Œæ‰“å°å‡ºæ¥è°ƒè¯•
    df_missed_info = df_missed_info.drop(need_drop_columns, axis=1)
    logger.info("(è°ƒè¯•)ä»¥ä¸‹è‚¡ç¥¨çš„æŸäº›ç‰¹å¾çš„'ç¼ºå¤±(NA)ç‡'ï¼Œè¶…è¿‡80%%ï¼Œ%d åª(éœ€è¦è¢«åˆ æ‰çš„è‚¡ç¥¨)ï¼š\n%r", len(df_missed_info), df_missed_info)
    # å‰”é™¤è¿™äº›é—®é¢˜è‚¡ç¥¨
    origin_stock_size = len(df_weekly.ts_code.unique())
    origin_data_size = df_weekly.shape[0]
    df_weekly = df_weekly[df_weekly.ts_code.apply(lambda x: x not in df_na_miss_codes)]
    logger.info("ä»%dåªè‚¡ç¥¨ä¸­å‰”é™¤äº†%dåªï¼Œå æ¯”%.1f%%ï¼›å‰”é™¤ç›¸å…³æ•°æ®%d=>%dè¡Œï¼Œå‰”é™¤å æ¯”%.2f%%",
                origin_stock_size,
                len(df_na_miss_codes),
                len(df_na_miss_codes) * 100 / origin_stock_size,
                origin_data_size,
                len(df_weekly),
                (origin_data_size - len(df_weekly)) * 100 / origin_data_size)

    """
    å»é™¤æå€¼+æ ‡å‡†åŒ–
    æ¯ä¸€åˆ—ï¼Œéƒ½å»æå€¼ï¼ˆTODOï¼šæ˜¯ä¸æ˜¯æŒ‰ç…§å„è‚¡è‡ªå·±çš„å€¼æ¥åšæ˜¯ä¸æ˜¯æ›´å¥½ï¼Ÿç°åœ¨æ˜¯æ‰€æœ‰çš„è‚¡ç¥¨ï¼‰
    ä¸­ä½æ•°å»æå€¼:
    - è®¾ç¬¬ T æœŸæŸå› å­åœ¨æ‰€æœ‰ä¸ªè‚¡ä¸Šçš„æš´éœ²åº¦åºåˆ—ä¸ºğ·ğ‘–
    - ğ·ğ‘€ä¸ºè¯¥åºåˆ—ä¸­ä½æ•°
    - ğ·ğ‘€1ä¸ºåºåˆ—|ğ·ğ‘– âˆ’ ğ·ğ‘€|çš„ä¸­ä½æ•°
    - åˆ™å°†åºåˆ—ğ·ğ‘–ä¸­æ‰€æœ‰å¤§äºğ·ğ‘€ + 5ğ·ğ‘€1çš„æ•°é‡è®¾ä¸ºğ·ğ‘€ + 5ğ·ğ‘€1
    - å°†åºåˆ—ğ·ğ‘–ä¸­æ‰€æœ‰å°äºğ·ğ‘€ âˆ’ 5ğ·ğ‘€1çš„æ•°é‡è®¾ä¸ºğ·ğ‘€ âˆ’ 5ğ·ğ‘€1
    """
    # æ¯åˆ—éƒ½æ±‚ä¸­ä½æ•°ï¼Œå’Œä¸­ä½æ•°ä¹‹å·®çš„ç»å¯¹å€¼çš„ä¸­ä½æ•°
    df_features_only = df_weekly[factor_names]
    # æ‰¾åˆ°æ¯ä¸€ä¸ªç‰¹å¾çš„ä¸­ä½å€¼
    df_median = df_features_only.median()
    # æ¯ä¸ªå€¼ï¼Œéƒ½å’Œä¸­ä½æ•°ç›¸å‡åï¼Œå–ç»å¯¹å€¼ï¼Œç„¶ååœ¨æ‰¾åˆ°ç»å¯¹å€¼ä»¬çš„ä¸­ä½æ•°ï¼Œè¿™ä¸ªå°±æ˜¯è¦é™å®šçš„èŒƒå›´å€¼
    df_scope = df_features_only.apply(lambda x: x - df_median[x.name]).abs().median()
    df_features_only = df_features_only.apply(lambda x: _scaller(x, df_median, df_scope))

    # æ ‡å‡†åŒ–ï¼š
    # å°†ä¸­æ€§åŒ–å¤„ç†åçš„å› å­æš´éœ²åº¦åºåˆ—å‡å»å…¶ç°åœ¨çš„å‡å€¼ã€é™¤ä»¥å…¶æ ‡å‡†å·®ï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„è¿‘ä¼¼æœä»N(0,1)åˆ†å¸ƒçš„åºåˆ—ã€‚
    scaler = StandardScaler()
    scaler.fit(df_features_only)
    df_weekly[factor_names] = scaler.transform(df_features_only)
    logger.info("å¯¹%dä¸ªç‰¹å¾è¿›è¡Œäº†æ ‡å‡†åŒ–(ä¸­ä½æ•°å»æå€¼)å¤„ç†ï¼š%d è¡Œ", len(factor_names), len(df_weekly))

    # å»é™¤æ‰€æœ‰çš„NANæ•°æ®(withç”¨æ¥æ˜¾ç¤ºæ‰€æœ‰èˆª)
    with pd.option_context('display.max_rows', None, 'display.max_columns', None):
        logger.info("NAç»Ÿè®¡ï¼šæ•°æ®ç‰¹å¾ä¸­çš„NANæ•°ï¼š\n%r", df_weekly[factor_names].isna().sum().sort_values())
    df_weekly = filter_invalid_data(df_weekly, factor_names)

    original_length = len(df_weekly)
    df_weekly.dropna(subset=factor_names + ['target'], inplace=True)
    logger.info("å»é™¤NANåï¼Œæ•°æ®å‰©ä½™è¡Œæ•°ï¼š%d=>%d è¡Œï¼Œå‰”é™¤äº†%.1f%%",
                original_length,
                len(df_weekly),
                (original_length - len(df_weekly)) * 100 / original_length)

    """
    å»é‡
    """
    original_length = len(df_weekly)
    df_weekly = df_weekly[~df_weekly[CODE_DATE].duplicated()].reset_index(drop=True)
    logger.info("å»é™¤é‡å¤è¡Œ(ts_code+trade_date)åï¼Œæ•°æ® %d => %d è¡Œï¼Œå‰”é™¤äº†%.1f%%",
                original_length,
                len(df_weekly),
                (original_length - len(df_weekly)) * 100 / original_length)

    # è¡Œä¸šä¸­æ€§åŒ–å¤„ç†
    if is_industry_market_neutral:
        start_time1 = time.time()
        industry_market_neutral = IndustryMarketNeutral(factor_names,
                                                        market_value_name='total_market_value_log',
                                                        industry_name='industry')
        industry_market_neutral.fit(df_weekly)
        df_weekly = industry_market_neutral.transform(df_weekly)
        time_elapse(start_time1, "è¡Œä¸šä¸­æ€§åŒ–å¤„ç†")

    # ä¿å­˜æœ€åçš„è®­ç»ƒæ•°æ®ï¼šts_codeã€trade_dateã€factorsã€target
    df_data = df_weekly[CODE_DATE + factor_names + ['target']]
    logger.info("ç‰¹å¾å¤„ç†ä¹‹åçš„æ•°æ®æƒ…å†µï¼š\n%r", df_data.describe())

    time_elapse(start_time, "â­ï¸ å…¨éƒ¨å› å­é¢„å¤„ç†å®Œæˆ")
    return df_weekly


def save_csv(name, df, start_date, end_date):
    csv_file_name = "data/{}_{}_{}_{}.csv".format(name, start_date, end_date, utils.now())
    df.to_csv(csv_file_name, header=True, index=False)  # ä¿ç•™åˆ—å
    logger.info("ä¿å­˜ %d è¡Œæ•°æ®åˆ°æ–‡ä»¶ï¼š%s", len(df), csv_file_name)


def filter_invalid_data(df, factor_names):
    for factor_name in factor_names:
        original_size = len(df)
        # å»æ‰é‚£äº›è¿™ä¸ªç‰¹å¾å…¨æ˜¯nançš„è‚¡ç¥¨
        valid_ts_codes = df.groupby('ts_code')[factor_name].count()[lambda x: x > 0].index
        df = df[df['ts_code'].isin(valid_ts_codes)]
        if len(df) != original_size:
            logger.info("å»é™¤ç‰¹å¾[%s]å…¨éƒ¨ä¸ºNançš„è‚¡ç¥¨æ•°æ®åï¼Œè¡Œæ•°å˜åŒ–ï¼š%d => %d",
                        factor_name, original_size, len(df))
    return df


def process_industry(df_basic):
    name_id_mapping_file = 'data/industry.json'

    # å…ˆå­˜ä¸€ä¸‹ä¸­æ–‡åï¼Œåé¢industryåˆ—ä¼šè¢«æ•°å­—æ›¿æ¢
    df_basic['industry_cn'] = df_basic['industry']
    # è¡Œä¸šçš„ç¼ºå¤±å€¼ä½¿ç”¨å…¶ä»–å¡«å……
    df_miss_industry = df_basic[df_basic.industry.isna()]
    logger.warning("ä»¥ä¸‹[%d]åªè‚¡ç¥¨ï¼Œå æ¯”%.1f%%, ç¼ºå°‘è¡Œä¸šä¿¡æ¯ï¼š\n%r",
                   len(df_miss_industry),
                   len(df_miss_industry) * 100 / len(df_basic),
                   df_miss_industry)
    df_basic.industry = df_basic.industry.fillna('å…¶ä»–')

    # åŠ è½½ æˆ– æ˜ å°„ï¼Œè¡Œä¸šçš„åå­—=>ID
    if os.path.exists(name_id_mapping_file):
        logger.info("è¡Œä¸šåç§°/IDæ˜ å°„æ–‡ä»¶[%s]å­˜åœ¨ï¼Œä½¿ç”¨å®ƒ", name_id_mapping_file)
        with open(name_id_mapping_file, 'r') as f:
            industry_to_number = json.load(f)
        # è½¬æˆæ•´å½¢
        for k, v in industry_to_number.items():
            industry_to_number[k] = int(v)
    else:
        # æ’ä¸ªåºï¼Œé˜²æ­¢åºå·å°†æ¥å†è¿è¡Œä¹±æ‰
        industry_names = df_basic.industry.sort_values().unique()

        # åå­—=>ID
        industry_to_number = {}
        for i, v in enumerate(industry_names):
            industry_to_number[v] = i + 1
        # ä¿å­˜ä¸‹æ¥æ˜ å°„
        with open(name_id_mapping_file, 'w', encoding='utf-8') as f:
            json.dump(industry_to_number, f, sort_keys=True, indent=4)
            logger.info("è¡Œä¸šç¼–ç ä¿¡æ¯ä¿å­˜åˆ°ï¼š%s", name_id_mapping_file)

    # è½¬æ¢æ•°æ®ä¸­çš„è¡Œä¸šï¼šåç§°=>ID
    df_basic.industry = df_basic.industry.map(industry_to_number)
    logger.debug("è¡Œä¸šIDæ˜ å°„ï¼š%r", industry_to_number)

    # è¿”å›å¤„ç†åçš„æ•°æ®
    return df_basic


"""
python -m mlstock.ml.data.factor_service -n 50 -d -s 20080101 -e 20220901
"""
if __name__ == '__main__':
    parser = argparse.ArgumentParser()

    # æ•°æ®ç›¸å…³çš„
    parser.add_argument('-s', '--start_date', type=str, default="20090101", help="å¼€å§‹æ—¥æœŸ")
    parser.add_argument('-e', '--end_date', type=str, default="20220801", help="ç»“æŸæ—¥æœŸ")
    parser.add_argument('-n', '--num', type=int, default=100000, help="è‚¡ç¥¨æ•°é‡ï¼Œè°ƒè¯•ç”¨")
    parser.add_argument('-in', '--industry_neutral', action='store_true', default=False, help="æ˜¯å¦åšè¡Œä¸šä¸­æ€§å¤„ç†")

    # å…¨å±€çš„
    parser.add_argument('-d', '--debug', action='store_true', default=True, help="æ˜¯å¦è°ƒè¯•")

    args = parser.parse_args()

    if args.debug:
        print("ã€è°ƒè¯•æ¨¡å¼ã€‘")
        utils.init_logger(file=True, log_level=logging.DEBUG)
    else:
        utils.init_logger(file=True, log_level=logging.INFO)

    calculate(args.start_date, args.end_date, args.num, args.industry_neutral)
